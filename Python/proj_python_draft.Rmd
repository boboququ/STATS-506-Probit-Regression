---
title: "Probit Regression in R, Python, Stata, and SAS"
author: "Shi Lan, Roya Talibova, Bo Qu,Jiehui Ding"
date: "2018/11/26"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---
```{r}
library(RStata)
```

##Model Introduction 
(tab content)


##Languages {.tabset}

###R

(tab content)

###Python

####Data Summary

First we start by loading the data into memory using the pandas package. The data is read in as a pandas dataframe, which is very similar to a R dataframe.

We also load numpy for array manipulation and we will be using the statsmodel package for Probit regression.

```
import pandas as pd
import numpy as np
from statsmodels.discrete.discrete_model import Probit

data = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/carData/Mroz.csv')
```

To print the first few rows of the data, we use the head function. This is similar to other languages such as R. 

```
print(data.head())

```

```

   Unnamed: 0  lfp  k5  k618  age   wc  hc       lwg        inc
0           1  yes   1     0   32   no  no  1.210165  10.910001
1           2  yes   0     2   30   no  no  0.328504  19.500000
2           3  yes   1     3   35   no  no  1.514128  12.039999
3           4  yes   0     3   34   no  no  0.092115   6.800000
4           5  yes   1     2   31  yes  no  1.524280  20.100000


```
####Data Cleaning

Since some of the data is read in as strings, we need to transform them into binary categorical data using the following code. We also drop the first column as it is read in with row numbers, which we do not need.

```
data = data.drop(data.columns[0], axis = 1)

data["lfp"] = data["lfp"] == "yes"
data["wc"] = data["wc"] == "yes"
data["hc"] = data["hc"] == "yes"

```

Looking at the data again we see:

```

    lfp  k5  k618  age     wc     hc       lwg        inc
0  True   1     0   32  False  False  1.210165  10.910001
1  True   0     2   30  False  False  0.328504  19.500000
2  True   1     3   35  False  False  1.514128  12.039999
3  True   0     3   34  False  False  0.092115   6.800000
4  True   1     2   31   True  False  1.524280  20.100000


```

####Summary Statistics

Before we begin our analysis here is a quick summary of the data. We use the function describe on our dataframe to generate some summary statistics.

```
print(data.describe())

```

This generates summary statistics for the continuous variables in our dataset. 

```
               k5        k618         age         lwg         inc
count  753.000000  753.000000  753.000000  753.000000  753.000000
mean     0.237716    1.353254   42.537849    1.097115   20.128965
std      0.523959    1.319874    8.072574    0.587556   11.634799
min      0.000000    0.000000   30.000000   -2.054124   -0.029000
25%      0.000000    0.000000   36.000000    0.818086   13.025000
50%      0.000000    1.000000   43.000000    1.068403   17.700001
75%      0.000000    2.000000   49.000000    1.399717   24.466000
max      3.000000    8.000000   60.000000    3.218876   96.000000

```


####Fitting Probit Regression


First we break our dataset into response variable and predictor variables. We will use lfp as our response variable and all the remaining variable as predictors. Then we use the statsmodels function to fit our Probit regression with our response variable and design matrix. The statsmodels package is unique from other languages and packages as it does not include an intercept term by default. This needs to be manually set. 


```
Y = data["lfp"]
X = data.drop(["lfp"], 1)
X = sm.add_constant(X)

model = Probit(Y, X.astype(float))
probit_model = model.fit()
print(probit_model.summary())

```

The following is the results of our regression. The statsmodels package automatically includes p values and confidence intervals for each coefficient. For those that are familiar with objects, the probit model is stored as a probit model object in Python. All operations with the model are invoked as model.member_function().


```
Optimization terminated successfully.
         Current function value: 0.601189
         Iterations 5
                          Probit Regression Results                           
==============================================================================
Dep. Variable:                    lfp   No. Observations:                  753
Model:                         Probit   Df Residuals:                      745
Method:                           MLE   Df Model:                            7
Date:                Fri, 07 Dec 2018   Pseudo R-squ.:                  0.1208
Time:                        01:40:27   Log-Likelihood:                -452.69
converged:                       True   LL-Null:                       -514.87
                                        LLR p-value:                 9.471e-24
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
const          1.9184      0.381      5.040      0.000       1.172       2.664
k5            -0.8747      0.114     -7.703      0.000      -1.097      -0.652
k618          -0.0386      0.040     -0.953      0.340      -0.118       0.041
age           -0.0378      0.008     -4.971      0.000      -0.053      -0.023
wc             0.4883      0.135      3.604      0.000       0.223       0.754
hc             0.0572      0.124      0.461      0.645      -0.186       0.300
lwg            0.3656      0.088      4.165      0.000       0.194       0.538
inc           -0.0205      0.005     -4.297      0.000      -0.030      -0.011
==============================================================================

```
It appears that all our predictors are statistically significant at level a except hc and k618. 

####Marginal Effects

To better understand why our predictors are significant, we attempt to study the marginal effects of our predictors. In this section, we will look at the predictors hc, wc, age, and k5. Unfortunately, there is no member function of the probit class to calculate the standard error of each prediction so this is ommited in this version. 

#####Group by Husband's College Attendance

First we group the data by hc. 

To better understand hc, we tabulate lfp and hc. This is done with the pandas crosstab function. 
```
print(pd.crosstab(data["lfp"], data["hc"], margins = True))
```

```
hc     False  True  All
lfp                    
False    207   118  325
True     251   177  428
All      458   295  753
```

Then we calculate adjusted predictions of lfp for two levels of the hc variable. Creating these adjusted predictors is not very clean in Python compared to other languages. We have to build the array from scratch.

```
hc_data0 = np.column_stack((
	1,
	np.mean(data["k5"]),
	np.mean(data["k618"]),
	np.mean(data["age"]),
	np.mean(data["wc"]),
	0,
	np.mean(data["lwg"]),
	np.mean(data["inc"])
	))
	
	hc_data1 = np.column_stack((
	1,
	np.mean(data["k5"]),
	np.mean(data["k618"]),
	np.mean(data["age"]),
	np.mean(data["wc"]),
	1,
	np.mean(data["lwg"]),
	np.mean(data["inc"])
	))
```

The results of running this prediction is as follows. 

```
print(probit_model.predict(hc_data0))
print(probit_model.predict(hc_data1))

```


```
[ 0.56938181]
[ 0.59171968]

```

We see that the marginal probability of husband being a collage graduate is 0.59, while the marginal probability of husband being a high school graduate is lower at 0.57.


#####Group by Wife's College Attendance

Similarly to hc, we tabulate our response variable lfp and predictor wc. We can see that there is a significant imbalance between labor force participation for women that are college graduates.
```
print(pd.crosstab(data["lfp"], data["wc"], margins = True))
```

```
wc     False  True  All
lfp                    
False    257    68  325
True     284   144  428
All      541   212  753
```

We also calculate the adjusted predictions of lfp for the two levels of wc variable, while keeping other variables at the mean. 

```

wc_data0 = np.column_stack((
	1,
	np.mean(data["k5"]),
	np.mean(data["k618"]),
	np.mean(data["age"]),
	0,
	np.mean(data["hc"]),
	np.mean(data["lwg"]),
	np.mean(data["inc"])
	))

wc_data1 = np.column_stack((
	1,
	np.mean(data["k5"]),
	np.mean(data["k618"]),
	np.mean(data["age"]),
	1,
	np.mean(data["hc"]),
	np.mean(data["lwg"]),
	np.mean(data["inc"])
	))


```

The result of running this prediction is as follows:


```
print(probit_model.predict(wc_data0))
print(probit_model.predict(wc_data1))
```

```
[ 0.52380974]
[ 0.70816505]
```

It appears that the marginal probability increases from 0.524 to 0.708 when women are college graduates compared to highschool graduates. 

#####Group by Age and Wife's College Attendance

We can predict the outcome variable by both age and women's college education. We use increments of 10 between 30 and 60 as our representative ages. 

```
wc_data0 = np.column_stack((
	np.repeat(1,4),
	np.repeat(np.mean(data["k5"]),4),
	np.repeat(np.mean(data["k618"]), 4),
	(30,40,50,60),
	np.repeat(0,4),
	np.repeat(np.mean(data["hc"]),4),
	np.repeat(np.mean(data["lwg"]),4),
	np.repeat(np.mean(data["inc"]),4)
	))

wc_data1 = np.column_stack((
	np.repeat(1,4),
	np.repeat(np.mean(data["k5"]),4),
	np.repeat(np.mean(data["k618"]), 4),
	(30,40,50,60),
	np.repeat(1,4),
	np.repeat(np.mean(data["hc"]),4),
	np.repeat(np.mean(data["lwg"]),4),
	np.repeat(np.mean(data["inc"]),4)
))
```
The results of running the prediction is as follows:
```
print(probit_model.predict(wc_data0))
print(probit_model.predict(wc_data1))
```
```
[ 0.70330952  0.5618684   0.41195181  0.27399923]
[ 0.84667045  0.74021954  0.6047985   0.45523423]
```

As we can see, the older a women gets, the less chance of her participating in the labor force. Regardless of age, women's college education always yields a higher probability of labor force participation.


#####Group by Number of Children 5 years or younger (K5)

Finally we look at the effects of number of children 5 years or younger. As before, here is a cross tabulation of this variable. 

```
print(pd.crosstab(data["lfp"], data["k5"], margins = True))
```
```
k5       0    1   2  3  All
lfp                        
False  231   72  19  3  325
True   375   46   7  0  428
All    606  118  26  3  753
```

Then we predict lfp at different levels of k5, while keeping other variables at their means. 

```
k5_data = np.column_stack((
	np.repeat(1,4),
	(0,1,2,3),
	np.repeat(np.mean(data["k618"]), 4),
	np.repeat(np.mean(data["age"]),4),
	np.repeat(np.mean(data["wc"]),4),
	np.repeat(np.mean(data["hc"]),4),
	np.repeat(np.mean(data["lwg"]),4),
	np.repeat(np.mean(data["inc"]),4)
	))
	
	print(probit_model.predict(k5_data))

```

```
[ 0.65730924  0.31932735  0.08942703  0.01324326]
```

We see that if there are three or more children under the age of 5, there is a 0.013 chance of a woman being in the work force, but when there is no children there is a 0.6573 chance. This is a good indication of the significance of this variable. 

#####Extension

Finally, an overall marginal effect can be observed by calling the get_margeff() method of the probit model class. This is unique to Python. Results are as follows.


```
mfx = probit_model.get_margeff()
print(mfx.summary())
```
```
 Probit Marginal Effects       
=====================================
Dep. Variable:                    lfp
Method:                          dydx
At:                           overall
==============================================================================
                dy/dx    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
k5            -0.2997      0.034     -8.726      0.000      -0.367      -0.232
k618          -0.0132      0.014     -0.955      0.340      -0.040       0.014
age           -0.0130      0.002     -5.219      0.000      -0.018      -0.008
wc             0.1673      0.045      3.696      0.000       0.079       0.256
hc             0.0196      0.042      0.461      0.645      -0.064       0.103
lwg            0.1253      0.029      4.311      0.000       0.068       0.182
inc           -0.0070      0.002     -4.451      0.000      -0.010      -0.004
==============================================================================

```


####Resources: 
http://www.statsmodels.org/0.6.1/examples/notebooks/generated/discrete_choice_overview.html
https://www.statsmodels.org/dev/generated/statsmodels.discrete.discrete_model.Probit.html
https://www.statsmodels.org/dev/examples/notebooks/generated/predict.html


###Stata

(tab content)


###SAS

(tab content)
